{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "data= pd.read_csv('data/20171015/data_bank.csv',converters={'jrid':str})\n",
    "##常用机器学习数据集加载 http://archive.ics.uci.edu/ml/index.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### 脏数据处理\n",
    "data = data.replace(['N',-1,'-1',-9999,'-9999'],np.nan)\n",
    "data.replace(['N',-1,'-1',-9999,'-9999'],np.nan,inplace=True)\n",
    "\n",
    "##### 缺失值处理\n",
    "data.isnull()       #元素级别的判断，把对应的所有元素的位置都列出来，元素为空或者NA就显示True，否则就是False\n",
    "data.isnull().any()　# 列级别的判断，只要该列有为空或者NA\n",
    "missing=data.columns[data.isnull().any()].tolist() # 将为空或者NA的列找出来\n",
    "data[missing].isnull().sum()  # 将列中为空或者NA的个数统计出来\n",
    "##在kaggle中有人这样处理缺失数据，如果数据的缺失达到15%，且并没有发现该变量有多大作用，就删除该变量\n",
    "data['column1']=data['column1'].fillna(0)  #.fillna(method='pad') .fillna(data['column1'].mean()) \n",
    "data.column1[data.column1.isnull()]=data.column1.dropna().mode().values #字符串类的数据剔除缺失值填充众数\n",
    "data['column1'].interpolate() # 使用插值来估计NaN 如果index是数字，可以设置参数method='value' ，如果是时间，可以设置method='time'\n",
    "\n",
    "\n",
    "##### 删除列\n",
    "del data['column1']       #删除列\n",
    "data['column1'].dropna()   #去掉为空值或者NA的元素\n",
    "data.drop(['column1'],axis=1)     #去掉column1列，不管空值与否\n",
    "df.drop(df.columns[[0,1]],axis=1,inplace=True)  #删除第1，2列，inplace=True表示直接就在内存中替换了，不用二次赋值生效。\n",
    "data.dropna(axis=0)         #删除带有空值的行\n",
    "data.dropna(axis=1)        #删除带有空值的列\n",
    "\n",
    "##### 字符串替换\n",
    "data['column1']=data['column1'].map({'RL':1,'RM':2,'RR':3,}).astype(int)\n",
    "\n",
    "\n",
    "##### 时间数据处理\n",
    "data['column1'] = (datetime.now() - pd.to_datetime(data['column1'].astype(str))).astype('timedelta64[D]')\n",
    "\n",
    "#### 数据转换\n",
    "data[\"column1\"] = np.log1p(data[\"column1\"])\n",
    "data= pd.get_dummies(data) # 将字符串特征列中的内容提出来作为新的特征\n",
    "\n",
    "#### 把内容为数值的特征列找出来\n",
    "numeric_feats =data.dtypes[data.dtypes != \"object\"].index \n",
    "\n",
    "\n",
    "#### 将偏斜度大于0.75的数值列做一个log转换，使之尽量符合正态分布，因为很多模型的假设数据是服从正态分布的\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "data[skewed_feats] = np.log1p(data[skewed_feats])\n",
    "\n",
    "#### 数据标准化\n",
    "from sklearn import preprocessing\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "standardized_X = preprocessing.scale(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计分析\n",
    "print(\"Skewness: %f\" % data['column'].skew())   # 列出数据的偏斜度\n",
    "print(\"Kurtosis: %f\" % data['column'].kurt())　 # 列出数据的峰度\n",
    "data['column1'].corr(data['column2'])           # 计算两个列的相关度\n",
    "data['SqrtColumn1']=np.sqrt(data['column1'])    # 将列的数值求根，并赋予一个新列\n",
    "data[['column1', 'column2']].groupby(['column2'], as_index=False).mean()  # f分组求均值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "normalized_X = preprocessing.normalize(X)  # 归一化\n",
    "standardized_X = preprocessing.scale(X)    # 标准化\n",
    "rbscale_X = preprocessing.robust_scale(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WOE特征离散化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "#逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "X_matrix_train = data_loan_train_woe[data_loan_train_woe.columns[2:]].as_matrix(columns=None)\n",
    "Y_matrix_train = data_loan_train_woe['overdue_m1'].as_matrix(columns=None)\n",
    "tuned_parameters = [{'penalty':['l1','l2'],'C':[0.001,0.01,0.1,1,10,100]}]\n",
    "clf = GridSearchCV(LogisticRegression(),tuned_parameters,cv=5,scoring='roc_auc')\n",
    "clf.fit(X_matrix_train,Y_matrix_train)\n",
    "clf.best_params_\n",
    "lr = LogisticRegression(**clf.best_params_)  #最优模型参数get\n",
    "lr_clf = lr.fit(X_matrix_train,Y_matrix_train)\n",
    "\n",
    "##knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(p=2)\n",
    "params = [{'penalty':['l1','l2'],'C':[0.001,0.01,0.1,10,100]}]\n",
    "## DT\n",
    "dt = tree.DecisionTreeClassifier(max_features=None,random_state=random)\n",
    "params = [{'criterion':['gini','entropy'],'max_depth':list(range(4,8,1)),'min_samples_leaf':list(range(1,8,1))}]\n",
    "## SVM \n",
    "svm = SVC(kernel='rbf',random_state=random,probability =True,class_weight='balanced')\n",
    "params = [{'C':[0.001,0.01,10,100]}]\n",
    "## RF\n",
    "rf = RandomForestClassifier(n_estimators=100,random_state=random)\n",
    "params = [{'n_estimators':[120,300],'max_depth':[5,8,15,None],'max_features':['sqrt','log2',None]}]\n",
    "## AdaBoost\n",
    "ab = AdaBoostClassifier(n_estimators=50,learning_rate=1.0,random_state=random)\n",
    "params = [{'base_estimator':[DecisionTreeClassifier(),LogisticRegression()],'n_estimators':list(range(50,100,10))}]\n",
    "## GBDT\n",
    "GBDT = GradientBoostingClassifier(loss='deviance',random_state=random)\n",
    "params = [{'n_estimators':[50,100],'learning_rate':[0.01,0.05,0.1],'subsample':[0.6,0.7,0.8,0.9,1.0],'max_depth':[5,8,15,25,30,None],\n",
    "               'min_samples_leaf':[1,2,5,10],'max_features':['log2','sqrt',None]}]\n",
    "\n",
    "## XGBoost\n",
    "xgbc = xgb.XGBClassifier(objective= 'binary:logistic',scale_pos_weight=1,seed=random) \n",
    "params = [{'gamma':[0.05,0.1,0.3,0.5,0.7,0.9,1.0],'max_depth':[5,8,15,25,30,None],\n",
    "           'min_samples_split':[2,5,10,15,100],'learning_rate':[0.01,0.015,0.025,0.05,0.1],\n",
    "           'min_samples_leaf':[1,2,5,10],'max_features':['log2','sqrt',None]}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估\n",
    "\n",
    "#### 1. 学习曲线\n",
    "#####  偏差现象,损失函数值均较大，欠拟合\n",
    "+ 增加假设模型的复杂度，欠拟合的本质是一些特征量在模型中未考虑。\n",
    "+ 减少lambda，因为模型还未达到该有的复杂度，真正本质的原因就在于lambda就是表示bias偏量大小的\n",
    "+ 在”偏差“现象中，通过增加训练样本的个数是无效的,是否加大训练样本的个数取决于是否有大的方差\n",
    "\n",
    "##### 方差现象-损失函数值均较小，但两者之间始终有间隔，过拟合\n",
    "+ 外在驱使，增加训练样本个数，提高了训练样本的复杂度\n",
    "+ 内在驱动，降低特征值的个数（比如多项式拟合中的幂次等级）\n",
    "+ 正规化处理，使用lambda权衡模型复杂度（Theta）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#学习曲线，了解模型的偏差方差信息\n",
    "#偏差现象-损失函数值均较大，欠拟合\n",
    "#方差现象-损失函数值均较小，但两者之间始终有间隔，过拟合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 模型保存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_clf, file_dir+'result/all_model/tianji_{}.model'.format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###函数库\n",
    "###计算KS\n",
    "def ks_calc(data,score_col,class_col):\n",
    "    '''\n",
    "    功能: 计算KS值，输出对应分割点和累计分布函数曲线图\n",
    "    输入值:\n",
    "    data: 二维数组或dataframe，包括模型得分和真实的标签\n",
    "    score_col: 一维数组或series，代表模型得分（一般为预测正类的概率）\n",
    "    class_col: 一维数组或series，代表真实的标签（{0,1}或{-1,1}）\n",
    "    输出值: \n",
    "    字典，键值关系为{'ks': KS值，'split': KS值对应节点，'fig': 累计分布函数曲线图}\n",
    "    '''\n",
    "    ks_dict = {}\n",
    "    Bad = data.ix[data[class_col[0]]==1,score_col[0]]\n",
    "    Good = data.ix[data[class_col[0]]==0, score_col[0]]\n",
    "    ks,pvalue = stats.ks_2samp(Bad.values,Good.values)\n",
    "    crossfreq = pd.crosstab(data[score_col[0]],data[class_col[0]])\n",
    "    crossdens = crossfreq.cumsum(axis=0) / crossfreq.sum()\n",
    "    crossdens['gap'] = abs(crossdens[0] - crossdens[1])\n",
    "    score_split = crossdens[crossdens['gap'] == crossdens['gap'].max()].index[0]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    crossdens[[0,1]].plot(kind='line',ax=ax)\n",
    "    ax.set_xlabel('%s' % score_col[0])\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('CDF Curve of Classified %s' % score_col[0])\n",
    "    plt.close()\n",
    "    ks_dict['ks'] = ks\n",
    "    ks_dict['split'] = score_split\n",
    "    ks_dict['ks_fig'] = fig\n",
    "    return ks_dict\n",
    "\n",
    "###计算AUC\n",
    "def auc_calc(data,score_col,class_col):\n",
    "    '''\n",
    "    功能: 计算AUC值，并输出ROC曲线\n",
    "    输入值:\n",
    "    data: 二维数组或dataframe，包括模型得分和真实的标签\n",
    "    score_col: 一维数组或series，代表模型得分（一般为预测正类的概率）\n",
    "    class_col: 一维数组或series，代表真实的标签（{0,1}或{-1,1}）\n",
    "    输出值: \n",
    "    字典，键值关系为{'auc': AUC值，'auc_fig': ROC曲线}\n",
    "    '''\n",
    "    auc_dict = {}\n",
    "#    fpr,tpr,threshold = roc_curve((1-data[class_col[0]]).ravel(),data[score_col[0]].ravel())   ###!!!!###\n",
    "    fpr,tpr,threshold = roc_curve((data[class_col[0]]).ravel(),data[score_col[0]].ravel())\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(fpr,tpr,color='b',label='ROC Curve (area=%0.3f)'%roc_auc,alpha=0.3)\n",
    "    plt.plot([0,1],[0,1],color='r',linestyle='--',alpha=0.3)\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve of %s'%score_col[0])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.close()\n",
    "    auc_dict['auc'] = roc_auc\n",
    "    auc_dict['auc_fig'] = fig\n",
    "    return auc_dict\n",
    "\n",
    "###交叉验证计算模型测试集上平均AUC和KS\n",
    "def fit_cv(X_matrix,Y_matrix,clf,cv,random):\n",
    "    '''\n",
    "    功能: 交叉验证计算模型测试集上平均AUC和KS\n",
    "    输入值:\n",
    "    X_matrix: 多维数组或dataframe，入模变量大宽表\n",
    "    Y_matrix: 一维数组或series，真实的标签\n",
    "    clf: 训练好的分类器\n",
    "    cv: int，交叉验证中测试集的个数\n",
    "    random: 随机排列的随机种子\n",
    "    输出值: \n",
    "    字典，键值关系为{'auc': 测试集上平均AUC值，'ks': 测试集上平均KS值}\n",
    "    '''\n",
    "    skf = StratifiedKFold(y=Y_matrix,n_folds=cv,random_state=random)\n",
    "    auc_list = []\n",
    "    ks_list = []\n",
    "    cv_dict = {}\n",
    "    for train_index,test_index in skf:\n",
    "        X_train,X_test = X_matrix[train_index],X_matrix[test_index]\n",
    "        Y_train,Y_test = Y_matrix[train_index],Y_matrix[test_index]\n",
    "        fit_data = pd.DataFrame(index=range(len(test_index)),columns=['Y','Prob'])\n",
    "        clf = clf.fit(X_train,Y_train)\n",
    "        fit_data['Y'] = Y_test\n",
    "        fit_data['Prob'] = clf.predict_proba(X_test)[:,1]\n",
    "        score_col = ['Prob']\n",
    "        class_col = ['Y']\n",
    "        auc_ = auc_calc(fit_data,score_col,class_col)['auc']\n",
    "        auc_list.append(auc_)\n",
    "        ks_ = ks_calc(fit_data,score_col,class_col)['ks']\n",
    "        ks_list.append(ks_)\n",
    "        cv_dict['ks'] = np.mean(ks_list)\n",
    "        cv_dict['auc'] = np.mean(auc_list)\n",
    "    return cv_dict\n",
    "\n",
    "###画好坏人分数分布对比直方图\n",
    "def plot_hist_score(y_true,y_score,close=True):\n",
    "    '''\n",
    "    功能: 画好坏人分数分布对比直方图\n",
    "    y_true: 一维数组或series，代表真实的标签（{0,1}或{-1,1}）\n",
    "    y_score: 一维数组或series，代表模型得分\n",
    "    close: 是否关闭图片\n",
    "    返回图片对象\n",
    "    '''\n",
    "    good = y_score[y_true==1]\n",
    "    bad = y_score[y_true==0]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(bad,bins=100,alpha=0.6,color='r',label='Bad')\n",
    "    ax.hist(good,bins=100,alpha=0.6,color='b',label='Good')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title('Histogram of Score in Good vs Bad')\n",
    "    if close:\n",
    "        plt.close('all')\n",
    "    return fig\n",
    "\n",
    "###学习曲线\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,color=\"r\")\n",
    "    \n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n",
    "    \n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.close()\n",
    "    fig.savefig(file_dir+'{}.png'.format(title))\n",
    "    \n",
    "    return plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
